{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "![s_スクリーンショット 2016-07-27 12.37.01.png](https://qiita-image-store.s3.amazonaws.com/0/77079/960963a7-9099-0e62-36b7-33736761ad8d.png)\n",
    "\n",
    "こんにちは、[Hironsan](http://qiita.com/Hironsan)です。\n",
    "\n",
    "顔認識は画像中に映った人を検知し、人物の識別を行う技術です。顔認識の用途としては、監視カメラのシステムに組み込んでセキュリティ向上に役立てたり、ロボットに組み込んで家族の顔を認識させたりすることがあげられます。\n",
    "\n",
    "今回はTensorFlowを使って畳み込みニューラルネットワークを構築し、既存のデータセットを使って顔認識器を作ってみます。\n",
    "\n",
    "# 対象読者\n",
    "\n",
    "* 畳み込みニューラルネットワーク(CNN)を知っている\n",
    "* TensorFlowでどう書くかはわからない\n",
    "\n",
    "CNNの理論については以下を見ればわかると思います。\n",
    "\n",
    "* [Convolutional Neural Networkとは何なのか](http://qiita.com/icoxfog417/items/5fd55fad152231d706c2)\n",
    "\n",
    "# 準備\n",
    "## TensorFlowのインストール\n",
    "TensorFlowのインストールは公式サイトが丁寧に解説しているのでそちらを参照してください。\n",
    "\n",
    "* [TensorFlowのインストール](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html)\n",
    "\n",
    "## データのダウンロード\n",
    "まずはデータセットを用意します。今回は以下の顔画像データセットを使います。\n",
    "\n",
    "* [Olivetti Faces](http://www.cs.nyu.edu/~roweis/data.html)\n",
    "\n",
    "このデータセットには40人分の画像が各10枚ずつ含まれています。各画像サイズは64x64でグレースケール画像です。\n",
    "![スクリーンショット 2016-07-25 22.30.04.png](https://qiita-image-store.s3.amazonaws.com/0/77079/c381389a-e8c0-27ab-d94c-6d689831a0fd.png)\n",
    "\n",
    "\n",
    "# 画像の読み込み\n",
    "データセットを用意したら、画像を読み込みます。\n",
    "[PyFaceRecognizer/example/input_data.py](https://github.com/Hironsan/PyFaceRecognizer/blob/master/example/input_data.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = len(labels_dense)\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    label_list = list(set(labels_dense))\n",
    "    for i, label in enumerate(labels_dense):\n",
    "        labels_one_hot[i][label_list.index(label)] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def extract_data(path):\n",
    "    mat = scipy.io.loadmat(path)\n",
    "    images = mat['faces'].T\n",
    "    images = np.array([np.reshape(cv2.resize(np.reshape(image, (64, 64)), (32, 32)), -1) for image in images])\n",
    "    labels = [i//10 for i in range(images.shape[0])]\n",
    "    labels_one_hot = dense_to_one_hot(labels, len(set(labels)))\n",
    "    return images, labels_one_hot\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 images,\n",
    "                 labels,\n",
    "                 dtype=dtypes.float32,\n",
    "                 reshape=True):\n",
    "\n",
    "        dtype = dtypes.as_dtype(dtype).base_dtype\n",
    "        if dtype not in (dtypes.uint8, dtypes.float32):\n",
    "            raise TypeError('Invalid image dtype %r, expected uint8 or float32' %dtype)\n",
    "\n",
    "        self._num_examples = images.shape[0]\n",
    "\n",
    "        if dtype == dtypes.float32:\n",
    "            # Convert from [0, 255] -> [0.0, 1.0].\n",
    "            images = images.astype(np.float32)\n",
    "            images = np.multiply(images, 1.0 / 255.0)\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size, fake_data=False):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "\n",
    "def read_data_sets(path, dtype=dtypes.float32, reshape=False):\n",
    "    images, labels = extract_data(path)\n",
    "    for i in range(images.shape[0]):\n",
    "        j = random.randint(i, images.shape[0]-1)\n",
    "        images[i], images[j] = images[j], images[i]\n",
    "        labels[i], labels[j] = labels[j], labels[i]\n",
    "\n",
    "    num_images = images.shape[0]\n",
    "\n",
    "    TRAIN_SIZE = int(num_images * 0.8)\n",
    "    VALIDATION_SIZE = int(num_images * 0.1)\n",
    "\n",
    "    train_images = images[:TRAIN_SIZE]\n",
    "    train_labels = labels[:TRAIN_SIZE]\n",
    "    validation_images = images[TRAIN_SIZE:TRAIN_SIZE+VALIDATION_SIZE]\n",
    "    validation_labels = labels[TRAIN_SIZE:TRAIN_SIZE+VALIDATION_SIZE]\n",
    "    test_images = images[TRAIN_SIZE+VALIDATION_SIZE:]\n",
    "    test_labels = labels[TRAIN_SIZE+VALIDATION_SIZE:]\n",
    "\n",
    "    train = DataSet(train_images, train_labels, dtype=dtype, reshape=reshape)\n",
    "    validation = DataSet(validation_images, validation_labels, dtype=dtype, reshape=reshape)\n",
    "    test = DataSet(test_images, test_labels, dtype=dtype, reshape=reshape)\n",
    "\n",
    "    return base.Datasets(train=train, validation=validation, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = read_data_sets('/Users/yahata/work/jupyter-notebooks/data/olivettifaces.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、datasetは学習データ、検証データ、テストデータを含んでいます。また、読み込んだ段階で画像サイズを32x32に縮小しています。\n",
    "\n",
    "\n",
    "<!--\n",
    "# セッションの開始\n",
    "Tensorflowでの計算は、高効率のC++バックエンドに依存しています。このバックエンドとの接続をセッションと呼びます。 TensorFlowプログラムの一般的な使用法は、まずグラフを作成し、セッションでそれを起動することです。\n",
    "ここでは、コードを構造化する方法についてTensorFlowをより柔軟にする便利なInteractiveSessionクラスを使用します。これを使えば、計算グラフを実行する間に、グラフを構築する操作をはさむことができます。このことは、iPythonのようなインタラクティブな状況で作業する場合、特に便利です。InteractiveSessionを使用しない場合は、セッションを開始し、グラフを起動する前に、全体のグラフを構築する必要があります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "この辺りにplaceholderを書いておく。\n",
    "-->\n",
    "\n",
    "# 畳み込みニューラルネットワークの構築\n",
    "畳み込みニューラルネットワーク(CNN)を用いて顔認識を行います。全体像としては以下のようになっています。\n",
    "\n",
    "<img width=\"781\" alt=\"スクリーンショット 2016-07-27 13.19.06.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/c5896d0e-b09d-b11f-e8f1-88801e478d5d.png\">\n",
    "\n",
    "各層のconv, pool, fcはそれぞれ畳み込み層、プーリング層、全結合層を表しています。関数欄のReLは正規化線形関数を表しています。パラメータを表にすると以下のようになります。\n",
    "\n",
    "| 層種・名称 | パッチ  | ストライド  | 出力マップサイズ  |  関数  |\n",
    "|---------|:------:|:-------:|:--------------:|:-----:|\n",
    "| data    |    -   |    -    |  32 x 32 x 1   |   -   |\n",
    "| conv1   |  5 x 5 |    1    |  32 x 32 x 32  |  ReL  |\n",
    "| pool1   |  2 x 2 |    2    |  16 x 16 x 32  |   -   |\n",
    "| conv2   |  5 x 5 |    1    |  16 x 16 x 64  |  ReL  |\n",
    "| pool2   |  2 x 2 |    2    |   8 x  8 x 64  |   -   |\n",
    "| fc3     |    -   |    -    |   1 x 1 x 1024 |  ReL  |\n",
    "| fc4     |    -   |    -    |   1 x 1 x 40   |  softmax |\n",
    "\n",
    "コードで書いてあげると以下のようになります。ほぼそのままですね。\n",
    "[PyFaceRecognizer/example/run.py](https://github.com/Hironsan/PyFaceRecognizer/blob/master/example/run.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練と評価\n",
    "inferenceにモデルのコードを書きました。次はモデルを訓練するためのコードを書いていきます。それがlossとtrainingです。lossではクロスエントロピーを計算し、trainingではAdamオプティマイザを使用してパラメータの更新を行っていきます。コードにすると以下の通りです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "以上で定義した、inference, loss, trainingを用いて顔認識を行います。訓練プロセスの100反復ごとにログを出力します。テストするときはドロップアウトさせないように、keep_peobを1.0にしています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x10cf7e2d0>> ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8862\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4fd5903d0ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yahata/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yahata/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yahata/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/yahata/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yahata/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def inference(input_placeholder, keep_prob):\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    x_image = tf.reshape(input_placeholder, [-1, 32, 32, 1])\n",
    "\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    W_fc1 = weight_variable([8 * 8 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 40])\n",
    "    b_fc2 = bias_variable([40])\n",
    "\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    return y_conv\n",
    "\n",
    "\n",
    "def loss(output, supervisor_labels_placeholder):\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(supervisor_labels_placeholder * tf.log(output), reduction_indices=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def training(loss):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 1024])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 40])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = inference(x, keep_prob)\n",
    "    loss = loss(output, y_)\n",
    "    training_op = training(loss)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1000):\n",
    "        batch = dataset.train.next_batch(40)\n",
    "        sess.run(training_op, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        if step % 100 == 0:\n",
    "            print(sess.run(loss, feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: dataset.test.images, y_: dataset.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 実行\n",
    "実行結果は下のような感じです。クロスエントロピーが下がっている様子が確認できます。\n",
    "\n",
    "```shell-session\n",
    "9.8893\n",
    "1.68918\n",
    "0.602403\n",
    "0.261183\n",
    "0.0490791\n",
    "0.0525591\n",
    "0.0133087\n",
    "0.0121071\n",
    "0.00673524\n",
    "0.00580989\n",
    "```\n",
    "\n",
    "# ソースコード\n",
    "以下のリポジトリからソースコードをダウンロードして動かすことができます。\n",
    "\n",
    "* [PyFaceRecognizer](https://github.com/Hironsan/PyFaceRecognizer)\n",
    "\n",
    "# おわりに\n",
    "畳み込みニューラルネットワークを用いて既存の顔データセットに対して顔認識を行ってみました。\n",
    "次はカメラからリアルタイムに取得した画像を使って、顔検知・顔認識をやってみたいと思います。\n",
    "\n",
    "# 参考資料\n",
    "\n",
    "* [TensorFlowのチュートリアル](https://www.tensorflow.org/)\n",
    "* [深層学習 (機械学習プロフェッショナルシリーズ)](https://www.amazon.co.jp/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E5%B2%A1%E8%B0%B7-%E8%B2%B4%E4%B9%8B/dp/4061529021/ref=sr_1_1?ie=UTF8&qid=1469600155&sr=8-1&keywords=%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
