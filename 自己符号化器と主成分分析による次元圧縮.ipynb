{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "\n",
    "次元圧縮手法として自己符号化器と主成分分析を実装しました．\n",
    "教科書として[『深層学習』](https://www.amazon.co.jp/深層学習-機械学習プロフェッショナルシリーズ-岡谷-貴之/dp/4061529021)と[『はじめてのパターン認識』](https://www.amazon.co.jp/はじめてのパターン認識-平井-有三/dp/4627849710)を使いました．\n",
    "\n",
    "本記事の構成\n",
    "\n",
    "* はじめに\n",
    "* 自己符号化器\n",
    "* 主成分分析\n",
    "* 2つの手法の関係\n",
    "* pythonでの実装\n",
    "    * auto encoder\n",
    "    * PCA\n",
    "* 結果\n",
    "* おわりに\n",
    "\n",
    "\n",
    "# 自己符号化器\n",
    "\n",
    "**自己符号化器（auto encoder）** とは，入力を訓練データとして使い，データをよく表す特徴を獲得するニューラルネットワークです．\n",
    "教師データを使わない教師なし学習に分類され，ニューラルネットワークの事前学習や初期値の決定などの利用されます．\n",
    "では，自己符号化器の説明に移ります．\n",
    "以下の図に示すような3層のネットワークを考えます．活性化関数は恒等写像関数 $g(a) = a$ です．\n",
    "\n",
    "<img width=\"420\" alt=\"network.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/82527/0bee98e3-bbf6-58f9-f196-d52fd3ef3e29.png\">\n",
    "\n",
    "\n",
    "中間層のユニットの値 $z_j$ および出力層のユニットの値 $y_k$ は，入力を順伝播させることで求められます．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z_j = \\sum_i w_{ji} x_i \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ y_k = \\sum_j \\tilde w_{kj} z_j $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "中間層のユニットの値を並べたベクトルを $\\boldsymbol z$，出力層のユニットの値を並べたベクトルを $\\boldsymbol y$，\n",
    "$w_{ji}$ を $( \\ j, i \\ )$ 成分に持つ行列を $\\boldsymbol W$ とすると以下のように表記できます．\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\boldsymbol z = \\boldsymbol W \\boldsymbol x \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ \\boldsymbol y = \\boldsymbol{\\tilde W} \\boldsymbol z $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "中間層 $\\boldsymbol z$ への変換を **符号化**，出力層 $\\boldsymbol y$ への変換を **復号化** と呼びます．\n",
    "自己符号化器では，復号化された $\\boldsymbol y$ が，入力 $\\boldsymbol x$ に近くなるように学習を行います．\n",
    "中間層のユニット数が入力次元よりも小さく，入力を再現できれば，次元を圧縮できたことになります．\n",
    "詳細な学習方法については[『pythonでニューラルネットワーク実装』](http://qiita.com/ta-ka/items/bcdfd2d9903146c51dcb)を参照してください．\n",
    "活性化関数が恒等写像関数なので，微分値は常に $1$ となり，計算がしやすいと思います．\n",
    "\n",
    "\n",
    "# 主成分分析\n",
    "\n",
    "**主成分分析（PCA）** とは，学習データ $\\boldsymbol x_i = (x_{i1}, ..., x_{id})^T$ の分散が最大になる方向への線形変換を求める手法です．\n",
    "$N$ 個のデータからなるデータ行列を $\\boldsymbol X = (\\boldsymbol x_1, ..., \\boldsymbol x_N)^T$，平均ベクトルを $\\boldsymbol{\\bar x} = (\\bar x_1, ..., \\bar x_d)^T$ とします．\n",
    "データ行列から平均ベクトルを引いたデータ行列 $\\boldsymbol{\\bar X}$，共分散行列 $\\boldsymbol \\Sigma$ は以下のようになります．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\boldsymbol{\\bar X} = (\\boldsymbol x_1 - \\boldsymbol{\\bar x}, ..., \\boldsymbol x_N - \\boldsymbol{\\bar x})^T \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ \\boldsymbol \\Sigma = Var\\bigl\\{\\boldsymbol{\\bar X}\\bigr\\} = \\frac{1}{N} \\boldsymbol{\\bar X}^T \\boldsymbol{\\bar X} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "データ行列 $\\boldsymbol{\\bar X}$ をある係数ベクトル $\\boldsymbol a_j$ で線形変換したものを $\\boldsymbol s_j$ すると，変換後のデータの分散は以下のようになります．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\boldsymbol s_j = (s_{1j}, ..., s_{Nj})^T \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ Var\\bigl\\{\\boldsymbol s_j \\bigr\\} \\varpropto \\boldsymbol s_j^T \\boldsymbol s_j = \\bigl(\\boldsymbol{\\bar X} \\boldsymbol a_j \\bigr)^T \\boldsymbol{\\bar X} \\boldsymbol a_j =  \\boldsymbol a_j^T \\boldsymbol{\\bar X}^T \\boldsymbol{\\bar X} \\boldsymbol a_j \\varpropto \\boldsymbol a_j^T Var\\bigl\\{\\boldsymbol{\\bar X}\\bigr\\} \\boldsymbol a_j $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "この分散を最大にする射影ベクトル $\\boldsymbol a_j$ は，ノルムを $1$ に制約したラグランジュ関数を最大化することで求められます．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E(\\boldsymbol a_j) = \\boldsymbol a_j^T Var\\bigl\\{\\boldsymbol{\\bar X}\\bigr\\} \\boldsymbol a_j - \\lambda (\\boldsymbol a_j^T \\boldsymbol a_j - 1) \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ \\frac{\\partial E(\\boldsymbol a_j)}{\\partial \\boldsymbol a_j} = 2 Var\\bigl\\{\\boldsymbol{\\bar X}\\bigr\\} \\boldsymbol a_j - 2 \\lambda \\boldsymbol a_j = 0 \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ Var\\bigl\\{\\boldsymbol{\\bar X}\\bigr\\} \\boldsymbol a_j = \\lambda \\boldsymbol a_j \\tag{*} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "式($\\ast$)は，元のデータの共分散行列に関する固有値問題を解くことで，分散を最大にする射影ベクトル $\\boldsymbol a_j$ が得られることを示しています．\n",
    "式($\\ast$)を解いて得られる固有値を $\\lambda_1 \\geq ... \\geq \\lambda_d$ とし，対応する固有ベクトルを $\\boldsymbol a_1, ..., \\boldsymbol a_d$ とします．\n",
    "共分散行列は実対称行列であるため，固有ベクトルは互いに直交します．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\boldsymbol a_i^T \\boldsymbol a_j = \\delta_{ij} = \\begin{cases} $$\n",
    "$$ 1 \\ ( \\ i = j \\ ) \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ 0 \\ ( \\ i \\ne j \\ ) $$\n",
    "$$ \\end{cases} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "最大固有値に対応する固有ベクトルにより線形変換された特徴量を第 $1$ 主成分，\n",
    "$k$ 番目の固有値に対応する固有ベクトルにより線形変換された特徴量を第 $k$ 主成分と呼びます．\n",
    "全分散量 $V_{total}$，第 $k$ 主成分の寄与率 $c_k$，第 $k$ 主成分までの累積寄与率 $r_k$ は以下のようになります．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V_{total} = \\sum_{i = 1}^d \\lambda_i \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ c_k = \\frac{\\lambda_k}{V_{total}} \\\\ $$\n",
    "$$ \\\\ $$\n",
    "$$ r_k = \\frac{\\sum_{i = 1}^k \\lambda_i}{V_{total}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2つの手法の関係\n",
    "\n",
    "固有値の降順に $\\boldsymbol \\Sigma$ の $D_y$ 個の固有ベクトルを選び，これを行ベクトルとして格納した行列を $\\boldsymbol U_{D_y}$ とします．\n",
    "入力データの次元数を $D_x$ とすると，$D_y < D_x$ のとき次元が圧縮されます．\n",
    "$\\boldsymbol U_{D_y}$ および $\\boldsymbol{\\bar x}$ は，以下の最小化問題の解 $(\\boldsymbol \\Gamma, \\boldsymbol \\xi) = (\\boldsymbol U_{D_y}, \\boldsymbol{\\bar x})$ となっています．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\min_{\\boldsymbol \\Gamma, \\boldsymbol \\xi} \\sum_{n = 1}^N \\bigl\\| \\ (\\boldsymbol x_n - \\boldsymbol \\xi) - \\boldsymbol \\Gamma^T \\boldsymbol \\Gamma(\\boldsymbol x_n - \\boldsymbol \\xi) \\ \\bigr\\|^2 \\tag{**} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "主成分分析では，$D_x$ 次元空間内の各データを，二乗距離が最小になるという意味で最もよく表す $D_y$ 次元部分空間を与えていると解釈できます．\n",
    "中間層および出力層の重みとバイアスを $\\boldsymbol W = \\boldsymbol \\Gamma, \\boldsymbol b = -\\boldsymbol \\Gamma \\boldsymbol \\xi, \\boldsymbol{\\tilde W} = \\boldsymbol \\Gamma^T, \\boldsymbol{\\tilde b} = \\boldsymbol \\xi$ とすると，自己符号化器の誤差関数は式($\\ast\\ast$)と一致します．\n",
    "つまり，自己符号化器では，$(\\boldsymbol \\Gamma, \\boldsymbol \\xi) = (\\boldsymbol U_{D_y}, \\boldsymbol{\\bar x})$ が誤差関数を最小化するネットワークのパラメータとなります．\n",
    "\n",
    "\n",
    "# pythonでの実装\n",
    "\n",
    "以下のように実装しました．\n",
    "自己符号化器，主成分分析を分けて掲載します．\n",
    "\n",
    "## auto encoder\n",
    "\n",
    "圧縮次元を $50$ として処理しています．\n",
    "学習率 $\\varepsilon = 0.00001$ で，$10000$ エポック回しています．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class AutoEncoder:\n",
    "\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        self.hidden_weight = numpy.random.randn(n_hidden, n_input + 1)\n",
    "        self.output_weight = numpy.random.randn(n_input, n_hidden + 1)\n",
    "\n",
    "    def fit(self, X, epsilon, epoch):\n",
    "        self.error = numpy.zeros(epoch)\n",
    "        N = X.shape[0]\n",
    "        for epo in range(epoch):\n",
    "            print u'epoch: %d' % epo\n",
    "            for x in X:\n",
    "                self.__update_weight(x, epsilon)\n",
    "\n",
    "            self.error[epo] = self.__calc_error(X)\n",
    "\n",
    "    def encode(self, X):\n",
    "        Z = numpy.zeros((X.shape[0], self.hidden_weight.shape[0]))\n",
    "        Y = numpy.zeros(X.shape)\n",
    "        for (i, x) in enumerate(X):\n",
    "            z, y = self.__forward(x)\n",
    "            Z[i, :] = z\n",
    "            Y[i, :] = y\n",
    "\n",
    "        return (Z, Y)\n",
    "\n",
    "    def __forward(self, x):\n",
    "        z = self.hidden_weight.dot(numpy.hstack((1, x)))\n",
    "        y = self.output_weight.dot(numpy.hstack((1, z)))\n",
    "\n",
    "        return (z, y)\n",
    "\n",
    "    def __update_weight(self, x, epsilon):\n",
    "        z, y = self.__forward(x)\n",
    "\n",
    "        # update output_weight\n",
    "        output_delta = y - x\n",
    "        self.output_weight -= epsilon * output_delta.reshape(-1, 1) * numpy.hstack((1, z))\n",
    "\n",
    "        # update hidden_weight\n",
    "        hidden_delta = self.output_weight[:, 1:].T.dot(output_delta)\n",
    "        self.hidden_weight -= epsilon * hidden_delta.reshape(-1, 1) * numpy.hstack((1, x))\n",
    "\n",
    "    def __calc_error(self, X):\n",
    "        N = X.shape[0]\n",
    "        err = 0.0\n",
    "        for x in X:\n",
    "            _, y = self.__forward(x)\n",
    "            err += (y - x).dot(y - x) / 2.0\n",
    "\n",
    "        return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data...\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import cv2\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# from auto_encoder import AutoEncoder\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print 'read data...'\n",
    "    mnist = fetch_mldata('MNIST original', data_home = '.')\n",
    "    _max = mnist.data[:, :-1].max()\n",
    "    X = mnist.data[:, :-1] * 1.0 / _max\n",
    "    input_size = X.shape[1]\n",
    "    hidden_size = 50\n",
    "    epsilon = 0.00001\n",
    "    epoch = 10000\n",
    "    stride = 50\n",
    "\n",
    "    print 'auto encoder init...'\n",
    "    auto = AutoEncoder(input_size, hidden_size)\n",
    "\n",
    "    print 'train...'\n",
    "    auto.fit(X[::stride], epsilon, epoch)\n",
    "\n",
    "    print 'encode...'\n",
    "    Z, Y = auto.encode(X[::stride])\n",
    "    for (i, y) in enumerate(Y * _max):\n",
    "        cv2.imwrite('result/%04d.png' % i, y.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PCA\n",
    "\n",
    "圧縮次元を $50$ として処理しています．\n",
    "射影ベクトルおよび復号化した画像を保存しています．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class PCA:\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.bar = self.__bar(X)\n",
    "        self.cov = self.__cov()\n",
    "        self.lamda, self.A, self.ccr = self.__solve_eigen_porblem()\n",
    "        self.S = self.__transformation()\n",
    "\n",
    "    def decode(self):\n",
    "        return self.S.dot(self.A.T)\n",
    "\n",
    "    def __bar(self, X):\n",
    "        return X - X.mean(axis = 0)\n",
    "\n",
    "    def __cov(self):\n",
    "        return numpy.cov(self.bar, rowvar = 0)\n",
    "\n",
    "    def __solve_eigen_porblem(self):\n",
    "        _lamda, _A = numpy.linalg.eig(self.cov)\n",
    "        lamda = _lamda[:self.n_components]\n",
    "        A = _A[:, :self.n_components]\n",
    "        ccr = lamda.sum() / _lamda.sum()\n",
    "\n",
    "        return (lamda, A, ccr)\n",
    "\n",
    "    def __transformation(self):\n",
    "        return self.bar.dot(self.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import os\n",
    "from pca import PCA\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print 'read data...'\n",
    "    mnist = fetch_mldata('MNIST original', data_home = '.')\n",
    "    X = mnist.data\n",
    "    n_components = 50\n",
    "    p_dir = 'projection/'\n",
    "    d_dir = 'decoded/'\n",
    "\n",
    "    print 'train...'\n",
    "    pca = PCA(n_components)\n",
    "    pca.fit(X)\n",
    "    print 'cumulative contribution ratio: %f' % pca.ccr\n",
    "\n",
    "    print 'decode...'\n",
    "    Xhat = pca.decode()\n",
    "\n",
    "    print 'save projection vector...'\n",
    "    if not os.path.exists(p_dir):\n",
    "        os.mkdir(p_dir)\n",
    "    for (i, a) in enumerate(pca.A.T):\n",
    "        fig, ax = pyplot.subplots()\n",
    "        heatmap = ax.pcolor(a.reshape(28, 28)[:, ::-1], cmap = pyplot.cm.RdYlBu)\n",
    "        pyplot.savefig(p_dir + '%04d.png' % i, dpi = 25)\n",
    "        pyplot.clf()\n",
    "\n",
    "    print 'save decoded images...'\n",
    "    if not os.path.exists(d_dir):\n",
    "        os.mkdir(d_dir)\n",
    "    for (i, xhat) in enumerate(Xhat[::50]):\n",
    "        cv2.imwrite(d_dir + '%04d.png' % i, xhat.reshape(28, 28))\n",
    "\n",
    "    n_components = 30\n",
    "    pca = PCA(n_components)\n",
    "    pca.fit(X)\n",
    "\n",
    "    Xhat = pca.A.dot(pca.S.T)\n",
    "    for (i, xhat) in enumerate(Xhat.T):\n",
    "        cv2.imwrite('result/%04d.png' % i, xhat.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 結果\n",
    "\n",
    "以下のような結果が得られました．\n",
    "\n",
    "### auto encoder\n",
    "\n",
    "復号化した画像\n",
    "\n",
    "<img width=\"720\" alt=\"decode_auto_encoder.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/82527/b76aae58-68f1-6245-c5a2-1cead979c23b.png\">\n",
    "\n",
    "\n",
    "\n",
    "誤差\n",
    "\n",
    "<img width=\"640\" alt=\"error_log.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/82527/77c69011-78bb-d7cb-8ddb-877b2a7d2f4f.png\">\n",
    "\n",
    "復号化した画像は見栄えのいいものを選びました．\n",
    "$50$ 次元に圧縮した特徴量から $784$ 次元へ復号化できています．\n",
    "誤差のグラフは横軸にエポック数，縦軸に誤差の値をとっています．\n",
    "誤差の桁の差が大きすぎたため，対数をとっています．\n",
    "厳密な誤差ではありませんが，単調に減少していることがわかります．．\n",
    "\n",
    "\n",
    "### PCA\n",
    "\n",
    "可視化した射影ベクトル\n",
    "\n",
    "<img width=\"735\" alt=\"projection_vector.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/82527/ddf13f2b-a1c5-52a1-1460-b6ccd2b6c563.png\">\n",
    "\n",
    "復号化した画像\n",
    "\n",
    "<img width=\"720\" alt=\"decode_pca.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/82527/8829b870-21fd-9748-087e-ab415bdd71af.png\">\n",
    "\n",
    "射影ベクトルは，固有値の大きい順番に，対応するベクトルを10個選択しました．\n",
    "左上の射影ベクトルが最も分散の大きい第1主成分へ射影します．\n",
    "復号化した画像は自己符号化器同様，良い結果が得られています．\n",
    "\n",
    "# おわりに\n",
    "\n",
    "自己符号化器および主成分分析によって次元を圧縮することができました．\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
