{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlowで株価予想シリーズ\n",
    "\n",
    "+ [0 - Google のサンプルコードを動かしてみる](http://qiita.com/akiraak/items/b27a5616a94cd64a8653)\n",
    "+ [1 - 終値が始値よりも高くなるかで判定してみる](http://qiita.com/akiraak/items/52aebc558ee312dba2b5)\n",
    "+ [2 - 日経平均225銘柄の株価予想正解率ランキング〜](http://qiita.com/akiraak/items/706f74eeba95ec32cf16)\n",
    "+ [3 - 日本3506銘柄の株価予想ランキング](http://qiita.com/akiraak/items/3e1ecbb9acfd872b944a)\n",
    "+ [4 - 実際に売買したら儲かるのかシミュレーションしてみる](http://qiita.com/akiraak/items/27d8e91982d3322873c3)\n",
    "+ [5 - 大きく上がると予想されたときだけ買ってみるシミュレーション](http://qiita.com/akiraak/items/fe45001ad011178fae5e)\n",
    "+ [6 - 学習データの項目を増やす！隠れ層のサイズも増やす！](http://qiita.com/akiraak/items/61def86cc8070d0fa4b2)\n",
    "+ [7 - 株価が何%上昇すると予測したら買えばいいのか？](http://qiita.com/akiraak/items/5d08587ac460c253492d)\n",
    "+ [8 - どの銘柄を買うか](http://qiita.com/akiraak/items/5e8a8c761f098a4a68fb)\n",
    "+ [9 - 年利6.79%](http://qiita.com/akiraak/items/8eec7ed4ec0b383f6592)\n",
    "\n",
    "# 前置き\n",
    "\n",
    "猫も杓子もディープラーニングディープラーニング。なにそれ美味いの？ って感じだけど、 2015年末に Google が書いた [「Machine Learning with Financial Time Series Data on Google Cloud Platform」](https://github.com/corrieelston/datalab/blob/master/FinancialTimeSeriesTensorFlow.ipynb) はとても美味しそうだ。Google がリリースした TensorFlow というディープラーニングツールで株価を予想しちゃうんだって！\n",
    "\n",
    "株価といっても世界中の株価指標を使ってS&P500というアメリカの株価指標が「上がるの？」「下がるの？」の２択を予想するもんらしい。株価指標ってのは日本でいうところの日経平均というやつで、複数の企業の株価を平均して値を出すもの。そしてヨーロッパの株価指数がどうだった？中国の株価指標がどうだった？じゃぁその後のアメリカの株価指標はこうなる！と予想する。\n",
    "\n",
    "で、 Google さんいわくこんな結果が出たそうだ。\n",
    "\n",
    "```\n",
    "Precision =  0.775862068966\n",
    "Recall =  0.625\n",
    "F1 Score =  0.692307692308\n",
    "Accuracy =  0.722222222222\n",
    "```\n",
    "\n",
    "Accuracy ってのが正解率で、72%で当たり外れを当てれるんだそうだ。Google すごいな。これ社内運用してるでしょ。経常利益をこれで増やしてるでしょ。\n",
    "\n",
    "ちなみに50%だと半分正解して半分外れるということになるけど、２択問題なのでそれはつまり全く当たらないという意味になる。50%から100%に近づけば予測が当たっているということだ。\n",
    "\n",
    "50%からの離脱をしよう。めざせ100%。そこに美味しい何かが待っている。\n",
    "\n",
    "# 実装コード\n",
    "\n",
    "\n",
    "で、妻の [ento](https://github.com/ento) に英語ドキュメントからコードを実装してもらい、それを正しく動作するように修正したコードが[こちら(github)](https://github.com/akiraak/tensorflow-stock-index/tree/1.0)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Code based on:\n",
    "https://github.com/corrieelston/datalab/blob/master/FinancialTimeSeriesTensorFlow.ipynb\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import urllib2\n",
    "from os import path\n",
    "import operator as op\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "DAYS_BACK = 3\n",
    "FROM_YEAR = '2000'\n",
    "EXCHANGES_DEFINE = [\n",
    "    #['DOW', '^DJI'],\n",
    "    #['FTSE', '^FTSE'],\n",
    "   # ['GDAXI', '^GDAXI'],\n",
    "    #['HSI', '^HSI'],\n",
    "    #['N225', '^N225'],\n",
    "    #['NASDAQ', '^IXIC'],\n",
    "    #['SP500', '^GSPC'],\n",
    "    #['SSEC', '000001.SS'],\n",
    "    [\"SP500\", \"CSV\"]\n",
    "]\n",
    "EXCHANGES_LABEL = [exchange[0] for exchange in EXCHANGES_DEFINE]\n",
    "\n",
    "Dataset = namedtuple(\n",
    "    'Dataset',\n",
    "    'training_predictors training_classes test_predictors test_classes')\n",
    "Environ = namedtuple('Environ', 'sess model actual_classes training_step dataset feature_data')\n",
    "\n",
    "\n",
    "def setupDateURL(urlBase):\n",
    "    now = datetime.date.today()\n",
    "    return urlBase.replace('__FROM_YEAR__', FROM_YEAR)\\\n",
    "            .replace('__TO_MONTH__', str(now.month - 1))\\\n",
    "            .replace('__TO_DAY__', str(now.day))\\\n",
    "            .replace('__TO_YEAR__', str(now.year))\n",
    "\n",
    "\n",
    "def fetchCSV(fileName, url):\n",
    "    if path.isfile(fileName):\n",
    "        print('fetch CSV for local: ' + fileName)\n",
    "        with open(fileName) as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        print('fetch CSV for url: ' + url)\n",
    "        csv = urllib2.urlopen(url).read()\n",
    "        with open(fileName, 'w') as f:\n",
    "            f.write(csv)\n",
    "        return csv\n",
    "\n",
    "\n",
    "def fetchYahooFinance(name, code):\n",
    "    fileName = 'index_%s.csv' % name\n",
    "    url = setupDateURL('http://chart.finance.yahoo.com/table.csv?s=%s&a=0&b=1&c=__FROM_YEAR__&d=__TO_MONTH__&e=__TO_DAY__&f=__TO_YEAR__&g=d&ignore=.csv' % code)\n",
    "    csv = fetchCSV(fileName, url)\n",
    "\n",
    "\n",
    "def fetchStockIndexes():\n",
    "    '''株価指標のデータをダウンロードしファイルに保存\n",
    "    '''\n",
    "    for exchange in EXCHANGES_DEFINE:\n",
    "        fetchYahooFinance(exchange[0], exchange[1])\n",
    "\n",
    "\n",
    "def load_exchange_dataframes():\n",
    "    '''EXCHANGESに対応するCSVファイルをPandasのDataFrameとして読み込む。\n",
    "    Returns:\n",
    "        {EXCHANGES[n]: pd.DataFrame()}\n",
    "    '''\n",
    "    return {exchange: load_exchange_dataframe(exchange)\n",
    "            for exchange in EXCHANGES_LABEL}\n",
    "\n",
    "\n",
    "def load_exchange_dataframe(exchange):\n",
    "    '''exchangeに対応するCSVファイルをPandasのDataFrameとして読み込む。\n",
    "    Args:\n",
    "        exchange: 指標名\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "    return pd.read_csv('index_{}.csv'.format(exchange)).set_index('Date').sort_index()\n",
    "\n",
    "\n",
    "def get_closing_data(dataframes):\n",
    "    '''各指標の終値カラムをまとめて1つのDataFrameに詰める。\n",
    "    Args:\n",
    "        dataframes: {key: pd.DataFrame()}\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "    closing_data = pd.DataFrame()\n",
    "    for exchange, dataframe in dataframes.items():\n",
    "        closing_data[exchange] = dataframe['Close']\n",
    "    closing_data = closing_data.fillna(method='ffill')\n",
    "    return closing_data\n",
    "\n",
    "\n",
    "def get_log_return_data(closing_data):\n",
    "    '''各指標について、終値を1日前との比率の対数をとって正規化する。\n",
    "    Args:\n",
    "        closing_data: pd.DataFrame()\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "\n",
    "    log_return_data = pd.DataFrame()\n",
    "    for exchange in closing_data:\n",
    "        # np.log(当日終値 / 前日終値) で前日からの変化率を算出\n",
    "        # 前日よりも上がっていればプラス、下がっていればマイナスになる\n",
    "        today_closing = closing_data[exchange]\n",
    "        yesterday_closing = closing_data[exchange].shift()\n",
    "        vary_ratio = np.log(today_closing / yesterday_closing)\n",
    "        #print(\"[VARY_RATIO] \" + str(today_closing) + \" / \" + str(yesterday_closing) + \" = \" + str(vary_ratio))\n",
    "        log_return_data[exchange] = vary_ratio\n",
    "\n",
    "    return log_return_data\n",
    "\n",
    "\n",
    "def build_training_data(log_return_data, target_exchange, max_days_back=DAYS_BACK, use_subset=None):\n",
    "    '''学習データを作る。分類クラスは、target_exchange指標の終値が前日に比べて上ったか下がったかの2つである。\n",
    "    また全指標の終値の、当日から数えてmax_days_back日前までを含めて入力データとする。\n",
    "    Args:\n",
    "        log_return_data: pd.DataFrame()\n",
    "        target_exchange: 学習目標とする指標名\n",
    "        max_days_back: 何日前までの終値を学習データに含めるか\n",
    "        use_subset (float): 短時間で動作を確認したい時用: log_return_dataのうち一部だけを学習データに含める\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "    # 「上がる」「下がる」の結果を計算\n",
    "    columns = []\n",
    "    for colname, exchange, operator in iter_categories(target_exchange):\n",
    "        columns.append(colname)\n",
    "        # 全ての XXX_positive, XXX_negative を 0 に初期化\n",
    "        log_return_data[colname] = 0\n",
    "        # XXX_positive の場合は >=  0 の全てのインデックスを\n",
    "        # XXX_negative の場合は < 0 の全てのインデックスを取得し、それらに 1 を設定する\n",
    "        indices = operator(log_return_data[exchange], 0)\n",
    "        log_return_data.ix[indices, colname] = 1\n",
    "\n",
    "    num_categories = len(columns)\n",
    "\n",
    "    # 各指標のカラム名を追加\n",
    "    for colname, _, _ in iter_exchange_days_back(target_exchange, max_days_back):\n",
    "        columns.append(colname)\n",
    "\n",
    "    '''\n",
    "    columns には計算対象の positive, negative と各指標の日数分のラベルが含まれる\n",
    "    例：[\n",
    "        'SP500_positive',\n",
    "        'SP500_negative',\n",
    "        'DOW_0',\n",
    "        'DOW_1',\n",
    "        'DOW_2',\n",
    "        'FTSE_0',\n",
    "        'FTSE_1',\n",
    "        'FTSE_2',\n",
    "        'GDAXI_0',\n",
    "        'GDAXI_1',\n",
    "        'GDAXI_2',\n",
    "        'HSI_0',\n",
    "        'HSI_1',\n",
    "        'HSI_2',\n",
    "        'N225_0',\n",
    "        'N225_1',\n",
    "        'N225_2',\n",
    "        'NASDAQ_0',\n",
    "        'NASDAQ_1',\n",
    "        'NASDAQ_2',\n",
    "        'SP500_1',\n",
    "        'SP500_2',\n",
    "        'SP500_3',\n",
    "        'SSEC_0',\n",
    "        'SSEC_1',\n",
    "        'SSEC_2'\n",
    "    ]\n",
    "    計算対象の SP500 だけ当日のデータを含めたらダメなので1〜3が入る\n",
    "    '''\n",
    "\n",
    "    # データ数をもとめる\n",
    "    max_index = len(log_return_data) - max_days_back\n",
    "    if use_subset is not None:\n",
    "        # データを少なくしたいとき\n",
    "        max_index = int(max_index * use_subset)\n",
    "\n",
    "    # 学習データを作る\n",
    "    training_test_data = pd.DataFrame(columns=columns)\n",
    "    for i in range(max_days_back + 10, max_index):\n",
    "        # 先頭のデータを含めるとなぜか上手くいかないので max_days_back + 10 で少し省く\n",
    "        values = {}\n",
    "        # 「上がる」「下がる」の答を入れる\n",
    "        for colname, _, _ in iter_categories(target_exchange):\n",
    "            values[colname] = log_return_data[colname].ix[i]\n",
    "        # 学習データを入れる\n",
    "        for colname, exchange, days_back in iter_exchange_days_back(target_exchange, max_days_back):\n",
    "            values[colname] = log_return_data[exchange].ix[i - days_back]\n",
    "            \n",
    "        training_test_data = training_test_data.append(values, ignore_index=True)\n",
    "\n",
    "    return num_categories, training_test_data\n",
    "\n",
    "\n",
    "def iter_categories(target_exchange):\n",
    "    '''分類クラス名とその値を計算するためのオペレーター関数を列挙する。\n",
    "    '''\n",
    "    for polarity, operator in [\n",
    "            ('positive', op.ge), # >=\n",
    "            ('negative', op.lt), # <\n",
    "    ]:\n",
    "        colname = '{}_{}'.format(target_exchange, polarity)\n",
    "        yield colname, target_exchange, operator\n",
    "\n",
    "\n",
    "def iter_exchange_days_back(target_exchange, max_days_back):\n",
    "    '''指標名、何日前のデータを読むか、カラム名を列挙する。\n",
    "    '''\n",
    "    for exchange in EXCHANGES_LABEL:\n",
    "        # SP500 の結果を予測するのに SP500 の当日の値が含まれてはいけないので１日づらす\n",
    "        start_days_back = 1 if exchange == target_exchange else 0\n",
    "        #start_days_back = 1 # N225 で行う場合は全て前日の指標を使うようにする\n",
    "        end_days_back = start_days_back + max_days_back\n",
    "        for days_back in range(start_days_back, end_days_back):\n",
    "            colname = '{}_{}'.format(exchange, days_back)\n",
    "            yield colname, exchange, days_back\n",
    "\n",
    "\n",
    "def split_training_test_data(num_categories, training_test_data):\n",
    "    '''学習データをトレーニング用とテスト用に分割する。\n",
    "    '''\n",
    "    # 先頭２つより後ろが学習データ\n",
    "    predictors_tf = training_test_data[training_test_data.columns[num_categories:]]\n",
    "    # 先頭２つが「上がる」「下がる」の答えデータ\n",
    "    classes_tf = training_test_data[training_test_data.columns[:num_categories]]\n",
    "\n",
    "    # 学習用とテスト用のデータサイズを求める\n",
    "    training_set_size = int(len(training_test_data) * 0.8)\n",
    "    test_set_size = len(training_test_data) - training_set_size\n",
    "\n",
    "    # 古いデータ0.8を学習とし、新しいデータ0.2がテストとなる\n",
    "    return Dataset(\n",
    "        training_predictors=predictors_tf[:training_set_size],\n",
    "        training_classes=classes_tf[:training_set_size],\n",
    "        test_predictors=predictors_tf[training_set_size:],\n",
    "        test_classes=classes_tf[training_set_size:],\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "    '''与えられたネットワークの正解率などを出力する。\n",
    "    '''\n",
    "    predictions = tf.argmax(model, 1)\n",
    "    actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "    tp_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, ones_like_actuals),\n",
    "                tf.equal(predictions, ones_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tn_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, zeros_like_actuals),\n",
    "                tf.equal(predictions, zeros_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fp_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, zeros_like_actuals),\n",
    "                tf.equal(predictions, ones_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fn_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, ones_like_actuals),\n",
    "                tf.equal(predictions, zeros_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tp, tn, fp, fn = session.run(\n",
    "        [tp_op, tn_op, fp_op, fn_op],\n",
    "        feed_dict\n",
    "    )\n",
    "\n",
    "    tpr = float(tp)/(float(tp) + float(fn))\n",
    "    fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "    accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "    recall = tpr\n",
    "    if (float(tp) + float(fp)):\n",
    "        precision = float(tp)/(float(tp) + float(fp))\n",
    "        f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "    else:\n",
    "        precision = 0\n",
    "        f1_score = 0\n",
    "\n",
    "    print('Precision = ', precision)\n",
    "    print('Recall = ', recall)\n",
    "    print('F1 Score = ', f1_score)\n",
    "    print('Accuracy = ', accuracy)\n",
    "\n",
    "\n",
    "def simple_network(dataset):\n",
    "    '''単純な分類モデルを返す。\n",
    "    '''\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "    num_predictors = len(dataset.training_predictors.columns)\n",
    "    num_classes = len(dataset.training_classes.columns)\n",
    "\n",
    "    # Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "    feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "    actual_classes = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define a matrix of weights and initialize it with some small random values.\n",
    "    weights = tf.Variable(tf.truncated_normal([num_predictors, num_classes], stddev=0.0001))\n",
    "    biases = tf.Variable(tf.ones([num_classes]))\n",
    "\n",
    "    # Define our model...\n",
    "    # Here we take a softmax regression of the product of our feature data and weights.\n",
    "    model = tf.nn.softmax(tf.matmul(feature_data, weights) + biases)\n",
    "\n",
    "    # Define a cost function (we're using the cross entropy).\n",
    "    cost = -tf.reduce_sum(actual_classes * tf.log(model))\n",
    "\n",
    "    # Define a training step...\n",
    "    # Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "    training_step = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    return Environ(\n",
    "        sess=sess,\n",
    "        model=model,\n",
    "        actual_classes=actual_classes,\n",
    "        training_step=training_step,\n",
    "        dataset=dataset,\n",
    "        feature_data=feature_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "株価指標データをダウンロードしcsvファイルに保存\n",
      "fetch CSV for local: index_SP500.csv\n",
      "株価指標データを読み込む\n",
      "終値を取得\n",
      "データを学習に使える形式に正規化\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "d = { \"target_exchange\": \"SP500\", \"steps\": 500, \"checkin\": 100, \"use_subset\": None, \"inspect\": False }\n",
    "args = argparse.Namespace(**d)\n",
    "\n",
    "print('株価指標データをダウンロードしcsvファイルに保存')\n",
    "fetchStockIndexes()\n",
    "\n",
    "print('株価指標データを読み込む')\n",
    "all_data  = load_exchange_dataframes()\n",
    "\n",
    "print('終値を取得')\n",
    "closing_data = get_closing_data(all_data)\n",
    "\n",
    "print('データを学習に使える形式に正規化')\n",
    "log_return_data = get_log_return_data(closing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答と学習データを作る\n"
     ]
    }
   ],
   "source": [
    "print('答と学習データを作る')\n",
    "num_categories, training_test_data = build_training_data(\n",
    "    log_return_data, args.target_exchange,\n",
    "    use_subset=args.use_subset)\n",
    "\n",
    "print('学習データをトレーニング用とテスト用に分割する')\n",
    "dataset = split_training_test_data(num_categories, training_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def smarter_network(dataset):\n",
    "    '''隠しレイヤー入りのもうちょっと複雑な分類モデルを返す。\n",
    "    '''\n",
    "    sess = tf.Session()\n",
    "\n",
    "    num_predictors = len(dataset.training_predictors.columns)\n",
    "    num_classes = len(dataset.training_classes.columns)\n",
    "\n",
    "    feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "    actual_classes = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    weights1 = tf.Variable(tf.truncated_normal([(DAYS_BACK * len(EXCHANGES_DEFINE)), 10], stddev=0.0001))\n",
    "    biases1 = tf.Variable(tf.ones([10]))\n",
    "\n",
    "    weights2 = tf.Variable(tf.truncated_normal([10, 5], stddev=0.0001))\n",
    "    biases2 = tf.Variable(tf.ones([5]))\n",
    "\n",
    "    weights3 = tf.Variable(tf.truncated_normal([5, 2], stddev=0.0001))\n",
    "    biases3 = tf.Variable(tf.ones([2]))\n",
    "\n",
    "    # This time we introduce a single hidden layer into our model...\n",
    "    hidden_layer_1 = tf.nn.relu(tf.matmul(feature_data, weights1) + biases1)\n",
    "    hidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, weights2) + biases2)\n",
    "    model = tf.nn.softmax(tf.matmul(hidden_layer_2, weights3) + biases3)\n",
    "\n",
    "    cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "    training_step = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    return Environ(\n",
    "        sess=sess,\n",
    "        model=model,\n",
    "        actual_classes=actual_classes,\n",
    "        training_step=training_step,\n",
    "        dataset=dataset,\n",
    "        feature_data=feature_data,\n",
    "    )\n",
    "\n",
    "print('器械学習のネットワークを作成')\n",
    "env = smarter_network(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(env, steps=30000, checkin_interval=5000):\n",
    "    '''学習をsteps回おこなう。\n",
    "    '''\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(env.model, 1),\n",
    "        tf.argmax(env.actual_classes, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    for i in range(1, 1 + steps):\n",
    "        env.sess.run(\n",
    "            env.training_step,\n",
    "            feed_dict=feed_dict(env, test=False),\n",
    "        )\n",
    "        if i % checkin_interval == 0:\n",
    "            print(i, env.sess.run(\n",
    "                accuracy,\n",
    "                feed_dict=feed_dict(env, test=False),\n",
    "            ))\n",
    "\n",
    "    tf_confusion_metrics(env.model, env.actual_classes, env.sess, feed_dict(env, True))\n",
    "\n",
    "def feed_dict(env, test=False):\n",
    "    '''学習/テストに使うデータを生成する。\n",
    "    '''\n",
    "    prefix = 'test' if test else 'training'\n",
    "    predictors = getattr(env.dataset, '{}_predictors'.format(prefix))\n",
    "    classes = getattr(env.dataset, '{}_classes'.format(prefix))\n",
    "    return {\n",
    "        env.feature_data: predictors.values,\n",
    "        env.actual_classes: classes.values.reshape(len(classes.values), len(classes.columns))\n",
    "    }\n",
    "\n",
    "print('学習') \n",
    "train(env, steps=args.steps, checkin_interval=args.checkin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解説\n",
    "\n",
    "解説はとりあえずは Google が書いた記事（[「Machine Learning with Financial Time Series Data on Google Cloud Platform」](https://github.com/corrieelston/datalab/blob/master/FinancialTimeSeriesTensorFlow.ipynb)）とコード内のコメントに任せたいと思います。 ほんとごめんね。\n",
    "\n",
    "データ操作に Pandas を使っているけど、「[Python Pandasでのデータ操作の初歩まとめ − 前半：データ作成＆操作編](http://qiita.com/hik0107/items/d991cc44c2d1778bb82e)」が参考になると思いま\n",
    "\n",
    "\n",
    "# 実行\n",
    "\n",
    "以下のコマンドでライブラリなどをインストールして\n",
    "\n",
    "```\n",
    "$ virtualenv --python=/usr/local/bin/python2.7 .pyenv\n",
    "$ . .pyenv/bin/activate\n",
    "$ pip install -r requirements.txt\n",
    "$ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n",
    "```\n",
    "\n",
    "これで実行\n",
    "\n",
    "```\n",
    "$ python goognet.py SP500\n",
    "...\n",
    "1000 0.546525\n",
    "2000 0.570082\n",
    "3000 0.81331\n",
    "4000 0.887809\n",
    "5000 0.916667\n",
    "6000 0.927856\n",
    "7000 0.93404\n",
    "8000 0.936396\n",
    "9000 0.935807\n",
    "10000 0.936396\n",
    "Precision =  0.966005665722\n",
    "Recall =  0.888020833333\n",
    "F1 Score =  0.925373134328\n",
    "Accuracy =  0.935217903416\n",
    "```\n",
    "\n",
    "0.93、、、93%の正解率。。。\n",
    "\n",
    "\n",
    "んなアホな。そんな正解率高いわけないだろ。。。\n",
    "\n",
    "\n",
    "# <del>おねがい</del>\n",
    "\n",
    "<del>\n",
    "データの確認をして問題なさそうだとは思った。でも、さすがにこの正解率は高すぎるだろう。S&P500と市場が開いている時間が近いDOWとNASDAQを省い（EXCHANGES_DEFINEでの２行をコメントアウトして実行）ても72%の正解率だ。\n",
    "\n",
    "\n",
    "`$ python goognet.py N225` と 日経225 の結果を得ると 65% の正解率。（N225で実行する場合は226行目と227行目のコードを入れ替える必要がある）\n",
    "\n",
    "結果がいいので実際に運用してみようと思うけど、誰か間違いに気づいたらこっそり教えてください。m(_ _)m\n",
    "</del>\n",
    "\n",
    "# 補足\n",
    "\n",
    "そんなに高いわけがなかった！DOWとNASDAQはSP500と同じ時間に開かれていて、学習データに含めることはそもそも不可能。同じ時間なので影響も受け合うから、ほぼ答が学習データに含まれていたため93%になったと思われる。再計算したところ73%でした。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
